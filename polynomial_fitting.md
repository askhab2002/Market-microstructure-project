## Как строится матрица мономов

- Есть набор точек $(x_n, y_n)$, $n = 1, \dots, N$, и максимальная степень полинома $d$.  
- Предполагается, что неявная зависимость задаётся полиномом  
  $$F(x, y) = \sum_k a_k\, x^{i_k} y^{j_k}, \quad 0 \le i_k + j_k \le d.$$
- Для всех пар степеней $(i, j)$ с $0 \le i + j \le d$ формируется список мономов:  
  $1, x, y, x^2, xy, y^2, \dots, x^d, x^{d-1}y, \dots, y^d$.  
- Матрица $M \in \mathbb{R}^{N \times K}$ (где $K$ — число мономов) строится так:
  - каждая строка соответствует одной точке $(x_n, y_n)$;  
  - каждый столбец соответствует одному мономy $x^{i_k} y^{j_k}$;  
  - элемент $M_{n,k} = x_n^{i_k} y_n^{j_k}$.  
- Вектор коэффициентов полинома $a \in \mathbb{R}^K$ задаёт $F$ в виде  
  $$F(x_n, y_n) = \sum_k a_k\, M_{n,k} = (M a)_n.$$

## Как формулируется задача и зачем нужна регуляризация

- Идея: точки $(x_n, y_n)$ лежат (приблизительно) на кривой $F(x, y) = 0$, то есть хотим, чтобы $F(x_n, y_n) \approx 0$ для всех $n$.  
- В матричной форме это означает, что вектор $M a$ должен быть близок к нулю:  
  $$M a \approx 0.$$
- Тривиальное решение $a = 0$ всегда даёт $M a = 0$, но не задаёт осмысленного полинома.  
- Поэтому напрямую решать $\min_a \|M a\|^2$ нельзя — нужно зафиксировать масштаб вектора $a$ (регуляризация).

## Задача минимизации отношения и связь с SVD

- Вводится нормировочное условие на коэффициенты и минимизируется «относительная» ошибка:
  $$J(a) = \frac{\|M a\|^2}{\|a\|^2}.$$
- Минимизируя $J(a)$ по всем ненулевым $a$, ищем вектор, для которого:
  - значения полинома $F(x_n, y_n)$ в среднем минимальны;  
  - длина вектора коэффициентов не может стать произвольно маленькой или большой.  
- Это эквивалентно решению обобщённой задачи на собственные значения для матрицы $M^\top M$:
  $$J(a) = \frac{a^\top M^\top M a}{a^\top a} \quad \Rightarrow \quad
  a \text{ — собственный вектор для минимального собственного значения } M^\top M.$$

## Как выглядит SVD и где там решение

- SVD-разложение матрицы $M$:
  $$M = U \Sigma V^\top,$$
  где:
  - $U \in \mathbb{R}^{N \times K}$ — ортонормальные левые сингулярные векторы;  
  - $\Sigma \in \mathbb{R}^{K \times K}$ — диагональная матрица сингулярных чисел $\sigma_1 \ge \dots \ge \sigma_K \ge 0$;  
  - $V \in \mathbb{R}^{K \times K}$ — ортонормальные правые сингулярные векторы.  
- Собственные значения матрицы $M^\top M$ равны квадратам сингулярных чисел $\sigma_k^2$, а собственные векторы — столбцы матрицы $V$.  
- Минимум $J(a)$ достигается на правом сингулярном векторе, соответствующем наименьшему сингулярному числу:
  - берём последний столбец $v_{\min}$ матрицы $V$ (или последнюю строку $V^\top$);  
  - задаём коэффициенты полинома $a = v_{\min}$.

## Почему это задаёт регуляризацию и не даёт тривиального решения

- Вектор $a$ из SVD автоматически нормирован: $\|a\| = 1$, то есть масштаб полинома фиксирован.  
- Поскольку $a$ выбирается так, чтобы $\|M a\|$ была минимальной при фиксированной норме, он:
  - не может быть нулевым (норма 1);  
  - не может расти без ограничения, чтобы «подогнать» отдельные точки — рост нормы запрещён нормировкой.  
- Таким образом, решение:
  - минимизирует алгебраическое расстояние полинома до данных;  
  - одновременно содержит «скрытую» $L_2$‑регуляризацию на коэффициенты (масштаб фиксирован);  
  - задаёт устойчивый неявный полином $F(x, y)$, кривая уровня $F(x,y)=0$ которого аппроксимирует наблюдаемую зависимость.
